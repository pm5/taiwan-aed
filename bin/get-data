#!/usr/bin/env python

import sys
from crawler.html import get_all_saved_ids
from crawler.data import save_json
from crawler.process import do_some, do_batch

def save_by_id(id):
    save_json(id)

def save_success(_):
    return True

def main(place_ids = []):
    if len(place_ids) > 0:
        do_some(place_ids, save_by_id, save_success,
                updated_message='item {item} saved',
                failed_message='extract information from {item} failed. skipped',
                sleep_sec=0)
    else:
        do_some(get_all_saved_ids(), save_by_id, save_success,
                updated_message='item {item} saved',
                failed_message='extract information from {item} failed. skipped',
                sleep_sec=0)

if __name__ == "__main__":
    main(sys.argv[1:])
