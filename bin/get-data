#!/usr/bin/env python

import sys
import os
import argparse

sys.path.append(os.path.join(os.curdir, 'lib'))
from crawler.html import get_all_html_ids
from crawler.data import save_json, get_csv, save_csv
from crawler.process import do_some, do_batch, do_collect

def save_json_by_id(args):
    def save(id):
        size = save_json(id, force=args.force)
        return size
    return save

def save_json_success(r):
    return r > 0

def yes(_): return True

def main(args):
    if len(args.place_ids) > 0:
        do_some(args.place_ids, save_json_by_id(args), save_json_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='extract information from {item} failed. skipped')
        do_collect(args.place_ids, get_csv, yes, save_csv,
                failed_message='extract information from {item} failed. skipped')
    else:
        do_some(get_all_html_ids(), save_json_by_id(args), save_json_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='extract information from {item} failed. skipped')
        do_collect(get_all_html_ids(), get_csv, yes, save_csv,
                failed_message='extract information from {item} failed. skipped')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Extract data from HTML.')
    parser.add_argument('place_ids', nargs='*', metavar='place_ids', type=int)
    parser.add_argument('-f --force', action='store_true', dest='force',
            default=False,
            help='Extract data even if already saved')
    args = parser.parse_args()
    main(args)
