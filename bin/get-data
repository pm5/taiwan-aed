#!/usr/bin/env python

import sys
from crawler.data import print_data, all_ids, save_json
import logging

logger = logging.getLogger()
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s'))
logger.addHandler(handler)
logger.setLevel(logging.INFO)

def do_on_all():
    do_on_many(all_ids())

def do_on_many(place_ids):
    for place_id in place_ids:
        try:
            save_json(place_id)
            logger.info('item {place_id} saved'.format(place_id=place_id))
        except:
            logger.warn('extract item {place_id} failed. skipped.'.format(
                place_id=place_id))

def main(*place_ids):
    if len(place_ids) > 0:
        do_on_many(place_ids)
    else:
        do_on_all()

if __name__ == "__main__":
    main(*sys.argv[1:])
