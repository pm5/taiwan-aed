#!/usr/bin/env python

import sys
import argparse
from crawler.html import get_all_html_ids
from crawler.data import save_json
from crawler.process import do_some, do_batch

def save_by_id(args):
    def save_json_by_id(id):
        size = save_json(id)
        return size
    def force_save_json_by_id(id):
        return save_json(id, force=True)
    return save_json_by_id if not args.force else force_save_json_by_id

def save_success(r):
    return r > 0

def main(args):
    if len(args.place_ids) > 0:
        do_some(args.place_ids, save_by_id(args), save_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='extract information from {item} failed. skipped',
                sleep_sec=0)
    else:
        do_some(get_all_html_ids(), save_by_id(args), save_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='extract information from {item} failed. skipped',
                sleep_sec=0)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Extract data from HTML.')
    parser.add_argument('place_ids', nargs='*', metavar='place_ids', type=int)
    parser.add_argument('-f --force', action='store_true', dest='force',
            default=False,
            help='Extract data even if already saved')
    args = parser.parse_args()
    main(args)
