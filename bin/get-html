#!/usr/bin/env python

import sys
from crawler.html import paginate, save_html
from crawler.process import do_some, do_batch

def get_by_id(id):
    size = save_html(id)
    return size

def get_success(size):
    return size > 0

def main(place_ids = []):
    if len(place_ids) > 0:
        do_some(place_ids, get_by_id, get_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='error on item {item}, not saved')
    else:
        do_batch(paginate(), get_by_id, get_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='error on item {item}, not saved')

if __name__ == "__main__":
    main(sys.argv[1:])
