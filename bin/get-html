#!/usr/bin/env python

import sys
import os
import argparse

sys.path.append(os.path.join(os.curdir, 'lib'))
from crawler.html import paginate, save_html
from crawler.process import do_some, do_batch

def get_by_id(args):
    def save_html_by_id(id):
        size = save_html(id)
        return size
    def force_save_htmL_by_id(id):
        return save_html(id, force=True)
    return save_html_by_id if not args.force else force_save_html_by_id

def get_success(size):
    return size > 0

def main(args):
    if len(args.place_ids) > 0:
        do_some(args.place_ids, get_by_id(args), get_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='error on item {item}, not saved')
    else:
        do_batch(paginate(), get_by_id(args), get_success,
                updated_message='item {item} saved',
                nochange_message='item {item} exists. skipped',
                failed_message='error on item {item}, not saved')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Crawl HTML')
    parser.add_argument('place_ids', nargs='*', metavar='place_ids', type=int)
    parser.add_argument('-f --force', action='store_true', dest='force',
            default=False,
            help='Get HTML even if already existed')
    args = parser.parse_args()
    main(args)
