#!/usr/bin/env python

import sys
import os
from subprocess import call
from os import access, chdir, R_OK
from shutil import copy

sys.path.append(os.path.join(os.curdir, 'lib'))
from crawler.data import data_dir, get_all_saved_ids

repository = 'git@github.com:pm5/taiwan-aed'
publish_dir = 'publish'

def main():
    """
    Publish new data.
    """
    if access(publish_dir, R_OK):
        chdir(publish_dir)
        call(['git', 'pull']);
        chdir('..')
    else:
        r = call(['git', 'clone', repository, '-b', 'data', publish_dir])
        if r == 128:
            call(['git', 'clone', repository, publish_dir])
            chdir(publish_dir)
            call(['git', 'checkout', '--orphan', 'data'])
            call(['git', 'rm', '-rf', '.'])
            chdir('..')

    for place_id in get_all_saved_ids():
        copy('{data_dir}/{place_id}.json'.format(data_dir=data_dir, place_id=place_id), publish_dir)
    # call(['rsync', '-a', '--include=*.json', '--exclude=.*', 'data/', publish_dir])

    chdir(publish_dir)
    call(['git', 'add', '-A', '.'])
    call(['git', 'commit', '-m', 'Published'])
    call(['git', 'push', '-u', 'origin', 'data'])

if __name__ == "__main__":
    main()
